
R version 3.6.1 (2019-07-05) -- "Action of the Toes"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ## Computation of DIC, EAIC, EBIC, LOOIC, WAIC for M3_MJM_MRI ##
> setwd('/pine/scr/h/a/haotian/MMFPCA_MRI/ADNI/')
> options(mc.cores = parallel::detectCores())
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.0.6     ✔ dplyr   0.8.3
✔ tidyr   1.0.0     ✔ stringr 1.4.0
✔ readr   1.3.1     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
> library(data.table)

Attaching package: ‘data.table’

The following objects are masked from ‘package:dplyr’:

    between, first, last

The following object is masked from ‘package:purrr’:

    transpose

> library(splines)
> library(lme4)
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

> library(survival)
> library(rstan)
Loading required package: StanHeaders
rstan (Version 2.21.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)

Attaching package: ‘rstan’

The following object is masked from ‘package:tidyr’:

    extract

> library(Matrix)
> library(refund)
> library(orthogonalsplinebasis)

Attaching package: ‘orthogonalsplinebasis’

The following object is masked from ‘package:stats’:

    integrate

> library(mfaces)
> library(loo)
This is loo version 2.4.1
- Online documentation and vignettes at mc-stan.org/loo
- As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. 

Attaching package: ‘loo’

The following object is masked from ‘package:rstan’:

    loo

> library(Amelia)
Loading required package: Rcpp
## 
## Amelia II: Multiple Imputation
## (Version 1.7.6, built: 2019-11-24)
## Copyright (C) 2005-2022 James Honaker, Gary King and Matthew Blackwell
## Refer to http://gking.harvard.edu/amelia/ for more information
## 
> library(sn)
Loading required package: stats4

Attaching package: ‘sn’

The following object is masked from ‘package:stats’:

    sd

> library(mgcv)
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:lme4’:

    lmList

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.8-34. For overview type 'help("mgcv-package")'.
> rstan_options(auto_write = FALSE)
> 
> set.seed(2020)
> 
> source('source_code/functions.R')
> source('source_code/f2.R')
> 
> long <- read.csv('dataset/ADNI_long_2.csv') 
> surv <- read.csv('dataset/ADNI_surv.csv')
> MRI.dat = read.csv('dataset/Surv.mri.dat.csv') %>% as.matrix()
> 
> N <- nrow(surv)
> J <- 5 ## number of longitudinal outcomes
> 
> ## FPCA2 wraps FPCA function
> FPCA2 <- function(dat){
+   S <- ncol(dat)
+   obsgrid <- 0:(S-1)
+   by <- 1
+   FPCA.obj <- FPCA(dat, obsgrid, pve = 0.99, by = by, L = 8)
+   return(FPCA.obj)
+ }
> Y1.FPCA <- FPCA2(MRI.dat)
> save(list = c('Y1.FPCA'), file = 'RData/FPCA_result_MJM_MRI.RData')
> 
> ## The minimum FPCA eigenvalues is 6, so we set Lb = 6 ##
> Lb <- ncol(Y1.FPCA$phi_est)
> create_B2 <- function(dat, FPCA.obj){
+   eigen.values <- FPCA.obj$values
+   S <- ncol(dat)
+   obsgrid <- 0:(S-1)/eigen.values[1]
+   by <- 1/eigen.values[1]
+   mu_est <- FPCA.obj$mu_est
+   phi_est <- FPCA.obj$phi_est/sqrt(by)
+   eigen.values <- eigen.values/eigen.values[1]
+   B <- create_B(dat, mu_est, phi_est, obsgrid, L = Lb, by = by)
+   return(B)
+ }
> 
> B1 <- create_B2(MRI.dat, Y1.FPCA)
> B2 <- B1
> B3 <- B1
> B4 <- B1
> 
> ## Refine the time grid ##
> tnew <- (0:101)/10
> tg <- length(tnew)
> 
> ## Impute the missing longitudinal outcomes, and average imputed datasets ##
> M <- 5
> long.impute <- amelia(x = long, m = M, idvars = 'ID', ts = 'Time', splinetime = 6)
-- Imputation 1 --

  1  2  3

-- Imputation 2 --

  1  2  3

-- Imputation 3 --

  1  2  3

-- Imputation 4 --

  1  2  3

-- Imputation 5 --

  1  2  3

> long.impute.dataset <- long.impute$imputations
> long.i <- long
> for (j in 1:J){
+   tmp.y <- rep(0, nrow(long))
+   for (m in 1:M){
+     tmp.y <- tmp.y + long.impute.dataset[[m]][, j+2]
+   }
+   long.i[, j+2] <- tmp.y/M
+ }
> 
> Y.list.o <- list(Y1 = long$y1, Y2 = long$y2, Y3 = long$y3, Y4 = long$y4, Y5 = long$y5)
> Y.list.i <- list(Y1 = long.i$y1, Y2 = long.i$y2, Y3 = long.i$y3, Y4 = long.i$y4, Y5 = long.i$y5)
> 
> ## get initial values of linear mixed model and survival model ##
> x_surv <- as.matrix(surv[, 4:7])
> l1 <- calc_init_long1(long$ID, long.i$y1, long$Time, long$Time, B = B1)
> l2 <- calc_init_long2(long$ID, long.i$y2, long$Time, re = l1[[2]], B = B1)
> l3 <- calc_init_long2(long$ID, long.i$y3, long$Time, re = l1[[2]], B = B1)
> l4 <- calc_init_long2(long$ID, long.i$y4, long$Time, re = l1[[2]], B = B1)
> l5 <- calc_init_long2(long$ID, long.i$y5, long$Time, re = l1[[2]], B = B1)
> v <- list(v2 = l2$fixed[3:4], v3 = l3$fixed[3:4], v4 = l4$fixed[3:4], v5 = l5$fixed[3:4])
> sigma_hat <- sqrt(c(l1$var.est[3], l2$var.est, l3$var.est, l4$var.est, l5$var.est))
> 
> fit.cox <- coxph(Surv(surv_time, status) ~ Age + Gender + Education + APOE4,
+                  data = surv, method = 'breslow')
> gamma_x_hat <- unname(fit.cox$coefficients)
> baseline.cumulative.hazard <- basehaz(fit.cox)
> 
> tau <- c(0, 4, 6, 8, max(surv$surv_time)+1)
> n.tau <- length(tau) - 1
> h.index <- matrix(0, nrow = N, ncol = n.tau)
> h.grid <- matrix(0, nrow = N, ncol = n.tau)
> 
> for (i in 1:N){
+   tmp.survtime <- surv$surv_time[i]
+   for (k in 1:n.tau){
+     h.index[i, k] <- ifelse(tmp.survtime>=tau[k] & tmp.survtime<tau[k+1], 1, 0)
+     h.grid[i, k] <- ifelse(tmp.survtime>=tau[k], min(tmp.survtime, tau[k+1]), tmp.survtime)
+   }
+ }
> h.grid <- cbind(0, h.grid)
> 
> logh0_hat <- rep(0, n.tau)
> for (k in 1:n.tau){
+   index1 <- min(which(baseline.cumulative.hazard$time>=tau[k]))
+   index2 <- max(which(baseline.cumulative.hazard$time<tau[k+1]))
+   lm.obj2 <- lm(hazard ~ time, data = baseline.cumulative.hazard[index1:index2, ])
+   logh0_hat[k] <- log(unname(lm.obj2$coefficients[2]))
+ }
> 
> ## Determine missingness
> Y_missing <- missing_determine(value = Y.list.o)
> mean_Y <- list(mean1 = rep(mean(long$y1, na.rm = T), Y_missing$len.missing[1]), 
+                mean2 = rep(mean(long$y2, na.rm = T), Y_missing$len.missing[2]),
+                mean3 = rep(mean(long$y3, na.rm = T), Y_missing$len.missing[3]), 
+                mean4 = rep(mean(long$y4, na.rm = T), Y_missing$len.missing[4]), 
+                mean5 = rep(mean(long$y5, na.rm = T), Y_missing$len.missing[5]))
> Y.list.new <- miss.index <- list()
> for (j in 1:J){
+   Y.list.new[[j]] <- Y_missing$new.value[[j]]
+   miss.index[[j]] <- Y_missing$missing.index[[j]]
+ }
> 
> start_end_index <- index_determine(long$ID)
> 
> ## Stan fit ##
> md = stan_model('source_code/M3_MJM_MRI.stan')
> stan_dat <- list(n = N, J = J, 
+                  nobs = nrow(long), nmiss = Y_missing$len.missing, 
+                  id_long = long$ID, 
+                  P_surv = ncol(x_surv), Lb = Lb, 
+                  time = long$Time, B1 = B1, 
+                  w = x_surv, 
+                  Y1 = Y.list.new[[1]], Y2 = Y.list.new[[2]], Y3 = Y.list.new[[3]],
+                  Y4 = Y.list.new[[4]], Y5 = Y.list.new[[5]],
+                  miss_index1 = miss.index[[1]], miss_index2 = miss.index[[2]], 
+                  miss_index3 = miss.index[[3]], miss_index4 = miss.index[[4]], 
+                  miss_index5 = miss.index[[5]],
+                  surv_time = surv$surv_time, status = surv$status,
+                  Ltau = n.tau, tau = tau[1:n.tau], 
+                  h_grid = h.grid, h_index = h.index, 
+                  zero = rep(0, 2),
+                  s1 = start_end_index$start, e1 = start_end_index$end)
> 
> rnd <- 1/3
> inits1 <- list(beta1 = rep(0, 2) + rnd, beta2 = rep(0, 2) + rnd, 
+                beta3 = rep(0, 2) + rnd, beta4 = rep(0, 2) + rnd, 
+                beta5 = rep(0, 2) + rnd, 
+                BX1 = rep(0, Lb) + rnd, BX2 = rep(0, Lb) + rnd, 
+                BX3 = rep(0, Lb) + rnd, BX4 = rep(0, Lb) + rnd, 
+                BX5 = rep(0, Lb) + rnd, BW = rep(0, Lb) + rnd, 
+                sigma_u = sqrt(l1$var.est[1:2]), 
+                rho = l1$corr.est,
+                v2 = rep(0, 2) + rnd, v3 = rep(0, 2) + rnd, 
+                v4 = rep(0, 2) + rnd, v5 = rep(0, 2) + rnd,
+                omega = g.inits.sd(sigma_hat),
+                logh0 = g.inits(logh0_hat), 
+                gamma = as.array(g.inits(gamma_x_hat)), 
+                alpha = c(1, -1, -1, -1, 1),
+                Y1_imp = mean_Y[[1]]+rnd, Y2_imp = mean_Y[[2]]+rnd, Y3_imp = mean_Y[[3]]+rnd, 
+                Y4_imp = mean_Y[[4]]+rnd, Y5_imp = mean_Y[[5]]+rnd)
> inits2 <- list(beta1 = rep(0, 2) - rnd, beta2 = rep(0, 2) - rnd, 
+                beta3 = rep(0, 2) - rnd, beta4 = rep(0, 2) - rnd, 
+                beta5 = rep(0, 2) - rnd, 
+                BX1 = rep(0, Lb) - rnd, BX2 = rep(0, Lb) - rnd, 
+                BX3 = rep(0, Lb) - rnd, BX4 = rep(0, Lb) - rnd, 
+                BX5 = rep(0, Lb) - rnd, BW = rep(0, Lb) - rnd, 
+                sigma_u = sqrt(l1$var.est[1:2]), 
+                rho = l1$corr.est,
+                v2 = rep(0, 2) - rnd, v3 = rep(0, 2) - rnd, 
+                v4 = rep(0, 2) - rnd, v5 = rep(0, 2) - rnd,
+                omega = g.inits.sd(sigma_hat),
+                logh0 = g.inits(logh0_hat), 
+                gamma = as.array(g.inits(gamma_x_hat)), 
+                alpha = c(1, -1, -1, -1, 1),
+                Y1_imp = mean_Y[[1]]-rnd, Y2_imp = mean_Y[[2]]-rnd, Y3_imp = mean_Y[[3]]-rnd, 
+                Y4_imp = mean_Y[[4]]-rnd, Y5_imp = mean_Y[[5]]-rnd)
> 
> inits <- list(c1 = inits1, c2 = inits2)
> pars <- c('beta1', 'beta2', 'beta3', 'beta4', 'beta5',
+           'BX1', 'BX2', 'BX3', 'BX4', 'BX5', 'BW', 
+           'sigma_u', 'rho', 'v2', 'v3', 'v4', 'v5',
+           'omega', 
+           'logh0', 'gamma', 'alpha',
+           'u', 'LL', 'log_lik',
+           'Y1_imp', 'Y2_imp', 'Y3_imp', 'Y4_imp', 'Y5_imp')
> fitStan <- sampling(md, data = stan_dat, iter = 3000, warmup = 2000, 
+                     chains = 2, thin=1, init = inits, pars = pars, seed = 2020,
+                     control = list(adapt_delta = 0.8, max_treedepth=10))

SAMPLING FOR MODEL 'M3_MJM_MRI' NOW (CHAIN 1).

SAMPLING FOR MODEL 'M3_MJM_MRI' NOW (CHAIN 2).
Chain 1: 
Chain 1: Gradient evaluation took 0.02 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 200 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: 
Chain 2: Gradient evaluation took 0.03 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 300 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 2: Iteration:    1 / 3000 [  0%]  (Warmup)
Chain 2: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 1: Iteration:  300 / 3000 [ 10%]  (Warmup)
Chain 2: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 3000 [ 20%]  (Warmup)
Chain 2: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 1: Iteration:  900 / 3000 [ 30%]  (Warmup)
Chain 2: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 1: Iteration: 1200 / 3000 [ 40%]  (Warmup)
Chain 2: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 1: Iteration: 1500 / 3000 [ 50%]  (Warmup)
Chain 2: Iteration: 1800 / 3000 [ 60%]  (Warmup)
Chain 1: Iteration: 1800 / 3000 [ 60%]  (Warmup)
Chain 2: Iteration: 2001 / 3000 [ 66%]  (Sampling)
Chain 1: Iteration: 2001 / 3000 [ 66%]  (Sampling)
Chain 2: Iteration: 2300 / 3000 [ 76%]  (Sampling)
Chain 1: Iteration: 2300 / 3000 [ 76%]  (Sampling)
Chain 2: Iteration: 2600 / 3000 [ 86%]  (Sampling)
Chain 1: Iteration: 2600 / 3000 [ 86%]  (Sampling)
Chain 2: Iteration: 2900 / 3000 [ 96%]  (Sampling)
Chain 1: Iteration: 2900 / 3000 [ 96%]  (Sampling)
Chain 2: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 27450.2 seconds (Warm-up)
Chain 2:                13517.5 seconds (Sampling)
Chain 2:                40967.7 seconds (Total)
Chain 2: 
Chain 1: Iteration: 3000 / 3000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 27698.7 seconds (Warm-up)
Chain 1:                13390.4 seconds (Sampling)
Chain 1:                41089.1 seconds (Total)
Chain 1: 
Warning messages:
1: There were 2000 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
2: Examine the pairs() plot to diagnose sampling problems
 
3: The largest R-hat is NA, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat 
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess 
5: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess 
> fname <- paste0('stan_fit/', 'M3_MJM_MRI_posterior.RData')
> save(list = 'fitStan', file = fname)
> # load(fname)
> summ <- summary(fitStan)$summary[1:(J*2 + Lb*6 + 2 + 1 + 2*(J-1) + J + n.tau + ncol(x_surv) + J), c(1, 3, 4, 8, 9, 10)]
> write.csv(summ, file = 'RData/summary_posterior_M3_MJM_MRI.csv')
> 
> ## Extract samples ##
> Q <- 2000
> beta_sample <- extract(fitStan, pars = c('beta1', 'beta2', 'beta3', 'beta4', 'beta5'))
> B_sample <- extract(fitStan, pars = c('BX1', 'BX2', 'BX3', 'BX4', 'BX5', 'BW'))
> sigma_u_sample <- extract(fitStan, par = 'sigma_u')$sigma_u
> rho_sample <- extract(fitStan, par = 'rho')$rho
> v_sample <- extract(fitStan, pars = c('v2', 'v3', 'v4', 'v5'))
> omega_sample <- extract(fitStan, par = 'omega')$omega
> logh0_sample <- extract(fitStan, 'logh0')$logh0
> gamma_sample <- extract(fitStan, pars = 'gamma')$gamma
> alpha_sample <- extract(fitStan, pars = 'alpha')$alpha
> u_sample <- extract(fitStan, pars = 'u')$u ## Q*L0*N
> LL_sample <- extract(fitStan, pars = 'LL')$LL
> log_lik_sample <- extract(fitStan, pars = 'log_lik')$log_lik
> Y_imp_sample <- extract(fitStan, pars = c('Y1_imp', 'Y2_imp', 'Y3_imp', 'Y4_imp', 'Y5_imp'))
> 
> ll_surv <- LL_sample
> 
> ## Calculation of log likelihood ##
> ll_long_full <- log_lik_sample
> 
> Dbar.long <- sum(-2*ll_long_full)/Q
> Dbar.surv <- sum(-2*ll_surv)/Q
> Dbar <- Dbar.long + Dbar.surv
> 
> np <- nrow(summ)
> EAIC <- Dbar + 2*np
> EBIC <- Dbar + log(N)*np
> 
> ## Compute LOOIC, WAIC ##
> ll_full <- ll_long_full + ll_surv
> rel_n_eff <- relative_eff(exp(ll_full), chain_id = rep(1:2, each = 1000))
> looic <- loo(ll_full, r_eff = rel_n_eff, cores = 4)$estimates[3, 1]
Warning message:
Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
 
> waic <- waic(ll_full)$estimates[3, 1]
Warning message:

706 (95.1%) p_waic estimates greater than 0.4. We recommend trying loo instead. 
> 
> l <- list(Dbar.long = Dbar.long, Dbar.surv = Dbar.surv,
+           np = np, EAIC = EAIC, EBIC = EBIC,
+           looic = looic, waic = waic)
> save(list = 'l', file = 'RData/summary_posterior_M3_MJM_MRI_MP.RData')
> 
> 
> proc.time()
     user    system   elapsed 
82117.091    81.843 41237.917 
